{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Getting Started with Pre-trained TSN Models on UCF101\n===========================================================\n\n`UCF101 <https://www.crcv.ucf.edu/data/UCF101.php>`_  is an action recognition dataset\nof realistic action videos, collected from YouTube. With 13,320 short trimmed videos\nfrom 101 action categories, it is one of the most widely used dataset in the research\ncommunity for benchmarking state-of-the-art video action recognition models.\n\n`TSN <https://arxiv.org/abs/1608.00859>`_ (Temporal Segment Network) is a widely adopted video\nclassification method. It is proposed to incorporate temporal information from an entire video.\nThe idea is straightforward: we can evenly divide the video into several segments,\nprocess each segment individually, obtain segmental consensus from each segment, and perform\nfinal prediction. TSN is more like a general algorithm, rather than a specific network architecture.\nIt can work with both 2D and 3D neural networks.\n\nIn this tutorial, we will demonstrate how to load a pre-trained TSN model from `gluoncv-model-zoo`\nand classify video frames from the Internet or your local disk into one of the 101 action classes.\n\nStep by Step\n------------\n\nWe will show two exmaples here. For simplicity, we first try out a pre-trained UCF101 model\non a single video frame. This is actually an image action recognition problem.\n\nFirst, please follow the `installation guide <../../index.html#installation>`__\nto install ``MXNet`` and ``GluonCV`` if you haven't done so yet.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nimport mxnet as mx\nfrom mxnet import gluon, nd, image\nfrom mxnet.gluon.data.vision import transforms\nfrom gluoncv.data.transforms import video\nfrom gluoncv import utils\nfrom gluoncv.model_zoo import get_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we download and show the example image:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "url = 'https://github.com/bryanyzhu/tiny-ucf101/raw/master/ThrowDiscus.png'\nim_fname = utils.download(url)\n\nimg = image.imread(im_fname)\n\nplt.imshow(img.asnumpy())\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In case you don't recognize it, the image is a man throwing discus. :)\n\nNow we define transformations for the image.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "transform_fn = transforms.Compose([\n    video.VideoCenterCrop(size=224),\n    video.VideoToTensor(),\n    video.VideoNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This transformation function does three things:\ncenter crop the image to 224x224 in size,\ntranspose it to ``num_channels*height*width``,\nand normalize with mean and standard deviation calculated across all ImageNet images.\n\nWhat does the transformed image look like?\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "img_list = transform_fn([img.asnumpy()])\nplt.imshow(np.transpose(img_list[0], (1,2,0)))\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Can't recognize anything? *Don't panic!* Neither do I.\nThe transformation makes it more \"model-friendly\", instead of \"human-friendly\".\n\nNext, we load a pre-trained VGG16 model. The VGG16 model is trained using TSN with three segments.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "net = get_model('vgg16_ucf101', nclass=101, pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that if you want to use InceptionV3 series model, please resize the image to have\nboth dimensions larger than 299 (e.g., 340x450) and change input size from 224 to 299\nin the transform function. Finally, we prepare the image and feed it to the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pred = net(nd.array(img_list[0]).expand_dims(axis=0))\n\nclasses = net.classes\ntopK = 5\nind = nd.topk(pred, k=topK)[0].astype('int')\nprint('The input video frame is classified to be')\nfor i in range(topK):\n    print('\\t[%s], with probability %.3f.'%\n          (classes[ind[i].asscalar()], nd.softmax(pred)[0][ind[i]].asscalar()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that our pre-trained model predicts this video frame\nto be ``throw discus`` action with high confidence.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next example is how to perform video action recognition, e.g., use the same\npre-trained model on an entire video.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we download the video and sample the video frames at a speed of 1 frame per second.\n\n.. raw:: html\n\n    <div align=\"center\">\n        <img src=\"../../_static/action_basketball_demo.gif\">\n    </div>\n\n    <br>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gluoncv.utils import try_import_cv2\ncv2 = try_import_cv2()\n\nurl = 'https://github.com/bryanyzhu/tiny-ucf101/raw/master/v_Basketball_g01_c01.avi'\nvideo_fname = utils.download(url)\n\ncap = cv2.VideoCapture(video_fname)\ncnt = 0\nvideo_frames = []\nwhile(cap.isOpened()):\n    ret, frame = cap.read()\n    cnt += 1\n    if ret and cnt % 25 == 0:\n        video_frames.append(frame)\n    if not ret: break\n\ncap.release()\nprint('We evenly extract %d frames from the video %s.' % (len(video_frames), video_fname))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we transform each video frame and feed them into the model.\nIn the end, we average the predictions from multiple video frames to get a reasonable prediction.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "video_frames_transformed = transform_fn(video_frames)\nfinal_pred = 0\nfor _, frame_img in enumerate(video_frames_transformed):\n    pred = net(nd.array(frame_img).expand_dims(axis=0))\n    final_pred += pred\nfinal_pred /= len(video_frames)\n\nclasses = net.classes\ntopK = 5\nind = nd.topk(final_pred, k=topK)[0].astype('int')\nprint('The input video is classified to be')\nfor i in range(topK):\n    print('\\t[%s], with probability %.3f.'%\n          (classes[ind[i].asscalar()], nd.softmax(final_pred)[0][ind[i]].asscalar()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that our pre-trained model predicts this video\nto be ``basketball`` action with high confidence.\nNote that, there are many ways to sample video frames and obtain a final video-level prediction.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next Step\n---------\n\nIf you would like to dive deeper into training TSN models on ``UCF101``,\nfeel free to read the next `tutorial on UCF101 <dive_deep_tsn_ucf101.html>`__.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}