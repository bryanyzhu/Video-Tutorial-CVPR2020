{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Dive Deep into Training TSN mdoels on UCF101\n==================================================\n\nThis is a video action recognition tutorial using Gluon CV toolkit, a step-by-step example.\nThe readers should have basic knowledge of deep learning and should be familiar with Gluon API.\nNew users may first go through `A 60-minute Gluon Crash Course <http://gluon-crash-course.mxnet.io/>`_.\nYou can `Start Training Now`_ or `Dive into Deep`_.\n\nStart Training Now\n~~~~~~~~~~~~~~~~~~\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Feel free to skip the tutorial because the training script is self-complete and ready to launch.\n\n    :download:`Download Full Python Script: train_recognizer.py<../../../scripts/action-recognition/train_recognizer.py>`\n\n    Example training command::\n\n        # Finetune a pretrained VGG16 model without using temporal segment network.\n        python train_recognizer.py --model vgg16_ucf101 --num-classes 101 --num-gpus 8 --lr-mode step --lr 0.001 --lr-decay 0.1 --lr-decay-epoch 30,60,80 --num-epochs 80\n\n        # Finetune a pretrained VGG16 model using temporal segment network.\n        python train_recognizer.py --model vgg16_ucf101 --num-classes 101 --num-gpus 8 --num-segments 3 --lr-mode step --lr 0.001 --lr-decay 0.1 --lr-decay-epoch 30,60,80 --num-epochs 80\n\n    For more training command options, please run ``python train_recognizer.py -h``\n    Please checkout the `model_zoo <../model_zoo/index.html#action_recognition>`_ for training commands of reproducing the pretrained model.</p></div>\n\n\nNetwork Structure\n-----------------\n\nFirst, let's import the necessary libraries into python.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n\nimport argparse, time, logging, os, sys, math\n\nimport numpy as np\nimport mxnet as mx\nimport gluoncv as gcv\nfrom mxnet import gluon, nd, init, context\nfrom mxnet import autograd as ag\nfrom mxnet.gluon import nn\nfrom mxnet.gluon.data.vision import transforms\n\nfrom gluoncv.data.transforms import video\nfrom gluoncv.data import UCF101\nfrom gluoncv.model_zoo import get_model\nfrom gluoncv.utils import makedirs, LRSequential, LRScheduler, split_and_load, TrainingHistory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Video action recognition is a classification problem.\nHere we pick a simple yet well-performing structure, ``vgg16_ucf101``, for the\ntutorial. In addition, we use the the idea of temporal segments (TSN) [Wang16]_\nto wrap the backbone VGG16 network for adaptation to video domain.\n\n`TSN <https://arxiv.org/abs/1608.00859>`_ is a widely adopted video\nclassification method. It is proposed to incorporate temporal information from an entire video.\nThe idea is straightforward: we can evenly divide the video into several segments,\nprocess each segment individually, obtain segmental consensus from each segment, and perform\nfinal prediction. TSN is more like a general algorithm, rather than a specific network architecture.\nIt can work with both 2D and 3D neural networks.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# number of GPUs to use\nnum_gpus = 1\nctx = [mx.gpu(i) for i in range(num_gpus)]\n\n# Get the model vgg16_ucf101 with temporal segment network, with 101 output classes, without pre-trained weights\nnet = get_model(name='vgg16_ucf101', nclass=101, num_segments=3)\nnet.collect_params().reset_ctx(ctx)\nprint(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Augmentation and Data Loader\n---------------------------------\n\nData augmentation for video is different from image. For example, if you\nwant to randomly crop a video sequence, you need to make sure all the video\nframes in this sequence undergo the same cropping process. We provide a\nnew set of transformation functions, working with multiple images.\nPlease checkout the `video.py <../../../gluoncv/data/transforms/video.py>`_ for more details.\nMost video data augmentation strategies used here are introduced in [Wang15]_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "transform_train = transforms.Compose([\n    # Fix the input video frames size as 256\u00d7340 and randomly sample the cropping width and height from\n    # {256,224,192,168}. After that, resize the cropped regions to 224 \u00d7 224.\n    video.VideoMultiScaleCrop(size=(224, 224), scale_ratios=[1.0, 0.875, 0.75, 0.66]),\n    # Randomly flip the video frames horizontally\n    video.VideoRandomHorizontalFlip(),\n    # Transpose the video frames from height*width*num_channels to num_channels*height*width\n    # and map values from [0, 255] to [0,1]\n    video.VideoToTensor(),\n    # Normalize the video frames with mean and standard deviation calculated across all images\n    video.VideoNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the transform functions, we can define data loaders for our\ntraining datasets.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Batch Size for Each GPU\nper_device_batch_size = 5\n# Number of data loader workers\nnum_workers = 8\n# Calculate effective total batch size\nbatch_size = per_device_batch_size * num_gpus\n\n# Set train=True for training the model. Here we set num_segments to 3 to enable TSN training.\ntrain_dataset = UCF101(train=True, num_segments=3, transform=transform_train)\nprint('Load %d training samples.' % len(train_dataset))\ntrain_data = gluon.data.DataLoader(train_dataset, batch_size=batch_size,\n                                   shuffle=True, num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimizer, Loss and Metric\n--------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Learning rate decay factor\nlr_decay = 0.1\n# Epochs where learning rate decays\nlr_decay_epoch = [30, 60, np.inf]\n\n# Stochastic gradient descent\noptimizer = 'sgd'\n# Set parameters\noptimizer_params = {'learning_rate': 0.001, 'wd': 0.0001, 'momentum': 0.9}\n\n# Define our trainer for net\ntrainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to optimize our model, we need a loss function.\nFor classification tasks, we usually use softmax cross entropy as the\nloss function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For simplicity, we use accuracy as the metric to monitor our training\nprocess. Besides, we record metric values, and will print them at the\nend of training.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_metric = mx.metric.Accuracy()\ntrain_history = TrainingHistory(['training-acc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training\n--------\n\nAfter all the preparations, we can finally start training!\nFollowing is the script.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>In order to finish the tutorial quickly, we only train for 3 epochs, and 100 iterations per epoch.\n  In your experiments, we recommend setting ``epochs=80`` for the full UCF101 dataset.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epochs = 3\nlr_decay_count = 0\n\nfor epoch in range(epochs):\n    tic = time.time()\n    train_metric.reset()\n    train_loss = 0\n\n    # Learning rate decay\n    if epoch == lr_decay_epoch[lr_decay_count]:\n        trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n        lr_decay_count += 1\n\n    # Loop through each batch of training data\n    for i, batch in enumerate(train_data):\n        # Extract data and label\n        data = split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n        label = split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n\n        # AutoGrad\n        with ag.record():\n            output = []\n            for _, X in enumerate(data):\n                X = X.reshape((-1,) + X.shape[2:])\n                pred = net(X)\n                output.append(pred)\n            loss = [loss_fn(yhat, y) for yhat, y in zip(output, label)]\n\n        # Backpropagation\n        for l in loss:\n            l.backward()\n\n        # Optimize\n        trainer.step(batch_size)\n\n        # Update metrics\n        train_loss += sum([l.mean().asscalar() for l in loss])\n        train_metric.update(label, output)\n\n        if i == 100:\n            break\n\n    name, acc = train_metric.get()\n\n    # Update history and print metrics\n    train_history.update([acc])\n    print('[Epoch %d] train=%f loss=%f time: %f' %\n        (epoch, acc, train_loss / (i+1), time.time()-tic))\n\n# We can plot the metric scores with:\ntrain_history.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can `Start Training Now`_.\n\nIf you would like to use a bigger 3D model (e.g., I3D) on a larger dataset (e.g., Kinetics400),\nfeel free to read the next `tutorial on Kinetics400 <demo_i3d_kinetics400.html>`__.\n\nReferences\n----------\n\n.. [Wang15] Limin Wang, Yuanjun Xiong, Zhe Wang, and Yu Qiao. \\\n    \"Towards Good Practices for Very Deep Two-Stream ConvNets.\" \\\n    arXiv preprint arXiv:1507.02159 (2015).\n\n.. [Wang16] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang and Luc Van Gool. \\\n    \"Temporal Segment Networks: Towards Good Practices for Deep Action Recognition.\" \\\n    In European Conference on Computer Vision (ECCV). 2016.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}