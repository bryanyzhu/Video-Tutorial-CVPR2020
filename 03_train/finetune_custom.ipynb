{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7. Fine-tuning SOTA video models on your own dataset\n=======================================================\n\nThis is a video action recognition tutorial using Gluon CV toolkit, a step-by-step example.\nThe readers should have basic knowledge of deep learning and should be familiar with Gluon API.\nNew users may first go through `A 60-minute Gluon Crash Course <http://gluon-crash-course.mxnet.io/>`_.\nYou can `Start Training Now`_ or `Dive into Deep`_.\n\nFine-tuning is an important way to obtain good video models on your own data when you don't have large annotated dataset or don't have the\ncomputing resources to train a model from scratch for your use case.\nIn this tutorial, we provide a simple unified solution.\nThe only thing you need to prepare is a text file containing the information of your videos (e.g., the path to your videos),\nwe will take care of the rest.\nYou can start fine-tuning from many popular pre-trained models (e.g., I3D, I3D-nonlocal, SlowFast) using a single command line.\n\nStart Training Now\n~~~~~~~~~~~~~~~~~~\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Feel free to skip the tutorial because the training script is self-complete and ready to launch.\n\n    :download:`Download Full Python Script: train_recognizer.py<../../../scripts/action-recognition/train_recognizer.py>`\n\n    For more training command options, please run ``python train_recognizer.py -h``\n    Please checkout the `model_zoo <../model_zoo/index.html#action_recognition>`_ for training commands of reproducing the pretrained model.</p></div>\n\n\nFirst, let's import the necessary libraries into python.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n\nimport argparse, time, logging, os, sys, math\n\nimport numpy as np\nimport mxnet as mx\nimport gluoncv as gcv\nfrom mxnet import gluon, nd, init, context\nfrom mxnet import autograd as ag\nfrom mxnet.gluon import nn\nfrom mxnet.gluon.data.vision import transforms\n\nfrom gluoncv.data.transforms import video\nfrom gluoncv.data import VideoClsCustom\nfrom gluoncv.model_zoo import get_model\nfrom gluoncv.utils import makedirs, LRSequential, LRScheduler, split_and_load, TrainingHistory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Custom DataLoader\n------------------\n\nWe provide a general dataloader for you to use on your own dataset. Your data can be stored in any hierarchy,\ncan be stored in either video format or already decoded to frames. The only thing you need\nto prepare is a text file, ``train.txt``.\n\nIf your data is stored in image format (already decoded to frames). Your ``train.txt`` should look like:\n\n::\n\n    video_001 200 0\n    video_001 200 0\n    video_002 300 0\n    video_003 100 1\n    video_004 400 2\n    ......\n    video_100 200 10\n\nThere are three items in each line, separated by spaces.\nThe first item is the path to your training videos, e.g., video_001.\nIt should be a folder containing the frames of video_001.mp4.\nThe second item is the number of frames in each video, e.g., 200.\nThe third item is the label of the videos, e.g., 0.\n\nIf your data is stored in video format. Your ``train.txt`` should look like:\n\n::\n\n    video_001.mp4 200 0\n    video_001.mp4 200 0\n    video_002.mp4 300 0\n    video_003.mp4 100 1\n    video_004.mp4 400 2\n    ......\n    video_100.mp4 200 10\n\nSimilarly, there are three items in each line, separated by spaces.\nThe first item is the path to your training videos, e.g., video_001.mp4.\nThe second item is the number of frames in each video. But you can put any number here\nbecause our video loader will compute the number of frames again automatically during training.\nThe third item is the label of that video, e.g., 0.\n\n\nOnce you prepare the ``train.txt``, you are good to go.\nJust use our general dataloader `VideoClsCustom <https://github.com/dmlc/gluon-cv/blob/master/gluoncv/data/kinetics400/classification.py>`_ to load your data.\n\nIn this tutorial, we will use UCF101 dataset as an example.\nFor your own dataset, you can just replace the value of ``root`` and ``setting`` to your data directory and your prepared text file.\nLet's first define some basics.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_gpus = 1\nctx = [mx.gpu(i) for i in range(num_gpus)]\ntransform_train = video.VideoGroupTrainTransform(size=(224, 224), scale_ratios=[1.0, 0.8], mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\nper_device_batch_size = 5\nnum_workers = 8\nbatch_size = per_device_batch_size * num_gpus\n\ntrain_dataset = VideoClsCustom(root=os.path.expanduser('~/.mxnet/datasets/ucf101/rawframes'),\n                               setting=os.path.expanduser('~/.mxnet/datasets/ucf101/ucfTrainTestlist/ucf101_train_split_1_rawframes.txt'),\n                               train=True,\n                               new_length=32,\n                               transform=transform_train)\nprint('Load %d training samples.' % len(train_dataset))\ntrain_data = gluon.data.DataLoader(train_dataset, batch_size=batch_size,\n                                   shuffle=True, num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Custom Network\n--------------\n\nYou can always define your own network architecture. Here, we want to show how to fine-tune on a pre-trained model.\nSince I3D model is a very popular network, we will use I3D with ResNet50 backbone trained on Kinetics400 dataset (i.e., ``i3d_resnet50_v1_kinetics400``) as an example.\n\nFor simple fine-tuning, people usually just replace the last classification (dense) layer to the number of classes in your dataset\nwithout changing other things. In GluonCV, you can get your customized model with one line of code.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "net = get_model(name='i3d_resnet50_v1_custom', nclass=101)\nnet.collect_params().reset_ctx(ctx)\nprint(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also provide other customized network architectures for you to use on your own dataset. You can simply change the ``dataset`` part in\nany pretrained model name to ``custom``, e.g., ``slowfast_4x16_resnet50_kinetics400`` to ``slowfast_4x16_resnet50_custom``.\n\nOnce you have the dataloader and network for your own dataset, the rest is the same as in previous tutorials.\nJust define the optimizer, loss and metric, and kickstart the training.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimizer, Loss and Metric\n--------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Learning rate decay factor\nlr_decay = 0.1\n# Epochs where learning rate decays\nlr_decay_epoch = [40, 80, 100]\n\n# Stochastic gradient descent\noptimizer = 'sgd'\n# Set parameters\noptimizer_params = {'learning_rate': 0.001, 'wd': 0.0001, 'momentum': 0.9}\n\n# Define our trainer for net\ntrainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to optimize our model, we need a loss function.\nFor classification tasks, we usually use softmax cross entropy as the\nloss function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For simplicity, we use accuracy as the metric to monitor our training\nprocess. Besides, we record metric values, and will print them at the\nend of training.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_metric = mx.metric.Accuracy()\ntrain_history = TrainingHistory(['training-acc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training\n--------\n\nAfter all the preparations, we can finally start training!\nFollowing is the script.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>In order to finish the tutorial quickly, we only fine tune for 3 epochs, and 100 iterations per epoch for UCF101.\n  In your experiments, you can set the hyper-parameters depending on your dataset.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epochs = 3\nlr_decay_count = 0\n\nfor epoch in range(epochs):\n    tic = time.time()\n    train_metric.reset()\n    train_loss = 0\n\n    # Learning rate decay\n    if epoch == lr_decay_epoch[lr_decay_count]:\n        trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n        lr_decay_count += 1\n\n    # Loop through each batch of training data\n    for i, batch in enumerate(train_data):\n        # Extract data and label\n        data = split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n        label = split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n\n        # AutoGrad\n        with ag.record():\n            output = []\n            for _, X in enumerate(data):\n                X = X.reshape((-1,) + X.shape[2:])\n                pred = net(X)\n                output.append(pred)\n            loss = [loss_fn(yhat, y) for yhat, y in zip(output, label)]\n\n        # Backpropagation\n        for l in loss:\n            l.backward()\n\n        # Optimize\n        trainer.step(batch_size)\n\n        # Update metrics\n        train_loss += sum([l.mean().asscalar() for l in loss])\n        train_metric.update(label, output)\n\n        if i == 100:\n            break\n\n    name, acc = train_metric.get()\n\n    # Update history and print metrics\n    train_history.update([acc])\n    print('[Epoch %d] train=%f loss=%f time: %f' %\n        (epoch, acc, train_loss / (i+1), time.time()-tic))\n\n# We can plot the metric scores with:\ntrain_history.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the training accuracy increase quickly.\nActually, if you look back tutorial 4 (Dive Deep into Training I3D mdoels on Kinetcis400) and compare the training curve,\nyou will see fine-tuning can achieve much better result using much less time.\nTry fine-tuning other SOTA video models on your own dataset and see how it goes.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}